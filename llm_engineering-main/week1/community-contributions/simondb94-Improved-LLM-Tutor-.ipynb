{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f87f094-c7da-4387-aa69-755b887954ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Improved-LLM-Tutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9387de1-7e71-46f4-b2f5-c384cc6051ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional, Union, Callable\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, HTML, update_display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import rich, install if not available\n",
    "try:\n",
    "    from rich.console import Console\n",
    "    from rich.markdown import Markdown as RichMarkdown\n",
    "    from rich.panel import Panel\n",
    "except ImportError:\n",
    "    !pip install rich\n",
    "    from rich.console import Console\n",
    "    from rich.markdown import Markdown as RichMarkdown\n",
    "    from rich.panel import Panel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71398f6e-c264-440c-bb86-fdbc1d47c713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "\n",
    "# Set up environment\n",
    "load_dotenv()\n",
    "openai = OpenAI()\n",
    "console = Console()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8593a84f-6d6e-44ee-b144-9ad88e8d1225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class LLMTutor:\n",
    "    \"\"\"\n",
    "    A class that provides tutoring functionality using multiple LLM models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n",
    "                 gpt_model: str = MODEL_GPT,\n",
    "                 llama_model: str = MODEL_LLAMA):\n",
    "        \"\"\"\n",
    "        Initialize the LLM Tutor with specified models and system prompt.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: The system prompt to use for the LLMs\n",
    "            gpt_model: The OpenAI GPT model to use\n",
    "            llama_model: The Ollama model to use\n",
    "        \"\"\"\n",
    "        self.system_prompt = system_prompt\n",
    "        self.gpt_model = gpt_model\n",
    "        self.llama_model = llama_model\n",
    "        self.history: List[Dict[str, Any]] = []\n",
    "        self.response_times = {'gpt': [], 'llama': []}\n",
    "        \n",
    "    def format_question(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Format the user's question with a standard prefix.\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question\n",
    "            \n",
    "        Returns:\n",
    "            Formatted question with prefix\n",
    "        \"\"\"\n",
    "        return f\"Please give a detailed explanation to the following question: {question}\"\n",
    "    \n",
    "    def create_messages(self, question: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Create the message structure for LLM API calls.\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question\n",
    "            \n",
    "        Returns:\n",
    "            List of message dictionaries\n",
    "        \"\"\"\n",
    "        formatted_question = self.format_question(question)\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": formatted_question}\n",
    "        ]\n",
    "    \n",
    "    def get_gpt_response(self, \n",
    "                        question: str, \n",
    "                        stream: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Get a response from the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question\n",
    "            stream: Whether to stream the response\n",
    "            \n",
    "        Returns:\n",
    "            The model's response as a string\n",
    "        \"\"\"\n",
    "        messages = self.create_messages(question)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                return self._stream_gpt_response(messages)\n",
    "            else:\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=self.gpt_model, \n",
    "                    messages=messages\n",
    "                )\n",
    "                elapsed = time.time() - start_time\n",
    "                self.response_times['gpt'].append(elapsed)\n",
    "                return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error with GPT model:[/bold red] {str(e)}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def _stream_gpt_response(self, messages: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Stream a response from the GPT model.\n",
    "        \n",
    "        Args:\n",
    "            messages: The messages to send to the model\n",
    "            \n",
    "        Returns:\n",
    "            The complete response as a string\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            stream = openai.chat.completions.create(\n",
    "                model=self.gpt_model, \n",
    "                messages=messages,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            response = \"\"\n",
    "            display_handle = display(Markdown(\"\"), display_id=True)\n",
    "            \n",
    "            for chunk in stream:\n",
    "                delta_content = chunk.choices[0].delta.content or ''\n",
    "                response += delta_content\n",
    "                # Clean the response for display\n",
    "                clean_response = response.replace(\"```python\", \"```\").replace(\"```\", \"\")\n",
    "                update_display(Markdown(clean_response), display_id=display_handle.display_id)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            self.response_times['gpt'].append(elapsed)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error streaming GPT response:[/bold red] {str(e)}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def get_llama_response(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Get a response from the Llama model.\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question\n",
    "            \n",
    "        Returns:\n",
    "            The model's response as a string\n",
    "        \"\"\"\n",
    "        messages = self.create_messages(question)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = ollama.chat(model=self.llama_model, messages=messages)\n",
    "            elapsed = time.time() - start_time\n",
    "            self.response_times['llama'].append(elapsed)\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error with Llama model:[/bold red] {str(e)}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def ask(self, question: str, models: List[str] = ['gpt', 'llama']) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Ask a question to one or more models.\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question\n",
    "            models: List of models to query ('gpt', 'llama', or both)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with model responses\n",
    "        \"\"\"\n",
    "        responses = {}\n",
    "        \n",
    "        # Store the question in history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'responses': {}\n",
    "        })\n",
    "        \n",
    "        # Get responses from requested models\n",
    "        if 'gpt' in models:\n",
    "            console.print(Panel(f\"[bold blue]Getting response from {self.gpt_model}...[/bold blue]\"))\n",
    "            gpt_response = self.get_gpt_response(question)\n",
    "            responses['gpt'] = gpt_response\n",
    "            self.history[-1]['responses']['gpt'] = gpt_response\n",
    "            \n",
    "        if 'llama' in models:\n",
    "            console.print(Panel(f\"[bold green]Getting response from {self.llama_model}...[/bold green]\"))\n",
    "            llama_response = self.get_llama_response(question)\n",
    "            responses['llama'] = llama_response\n",
    "            self.history[-1]['responses']['llama'] = llama_response\n",
    "            display(Markdown(f\"## {self.llama_model} Response\\n{llama_response}\"))\n",
    "            \n",
    "        return responses\n",
    "    \n",
    "    def compare_responses(self, question: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Compare responses from different models side by side.\n",
    "        \n",
    "        Args:\n",
    "            question: Optional specific question to compare responses for\n",
    "        \"\"\"\n",
    "        if question:\n",
    "            responses = self.ask(question)\n",
    "        else:\n",
    "            # Use the most recent question from history\n",
    "            if not self.history:\n",
    "                console.print(\"[bold red]No questions in history to compare[/bold red]\")\n",
    "                return\n",
    "            responses = self.history[-1]['responses']\n",
    "            question = self.history[-1]['question']\n",
    "        \n",
    "        # Create HTML for side-by-side comparison\n",
    "        html = f\"\"\"\n",
    "        <div style=\"display: flex; width: 100%;\">\n",
    "            <div style=\"flex: 1; padding: 10px; border: 1px solid #ddd; border-radius: 5px; margin-right: 10px;\">\n",
    "                <h3 style=\"color: #4285F4;\">{self.gpt_model}</h3>\n",
    "                <div style=\"white-space: pre-wrap;\">{responses.get('gpt', 'No response')}</div>\n",
    "            </div>\n",
    "            <div style=\"flex: 1; padding: 10px; border: 1px solid #ddd; border-radius: 5px;\">\n",
    "                <h3 style=\"color: #34A853;\">{self.llama_model}</h3>\n",
    "                <div style=\"white-space: pre-wrap;\">{responses.get('llama', 'No response')}</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(html))\n",
    "    \n",
    "    def show_performance_metrics(self) -> None:\n",
    "        \"\"\"\n",
    "        Display performance metrics for the models.\n",
    "        \"\"\"\n",
    "        if not self.response_times['gpt'] and not self.response_times['llama']:\n",
    "            console.print(\"[bold yellow]No performance data available yet[/bold yellow]\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame for metrics\n",
    "        data = {\n",
    "            'Model': [],\n",
    "            'Response Time (s)': []\n",
    "        }\n",
    "        \n",
    "        for model, times in self.response_times.items():\n",
    "            for t in times:\n",
    "                data['Model'].append(model)\n",
    "                data['Response Time (s)'].append(t)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = df.groupby('Model')['Response Time (s)'].agg(['mean', 'min', 'max', 'count'])\n",
    "        \n",
    "        # Display statistics\n",
    "        console.print(\"\\n[bold]Performance Statistics:[/bold]\")\n",
    "        console.print(stats)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Box plot\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        df.boxplot(column='Response Time (s)', by='Model', ax=ax)\n",
    "        plt.title('Response Time Distribution')\n",
    "        plt.suptitle('')\n",
    "        \n",
    "        # Bar chart for average times\n",
    "        ax = plt.subplot(1, 2, 2)\n",
    "        stats['mean'].plot(kind='bar', ax=ax, color=['#4285F4', '#34A853'])\n",
    "        plt.title('Average Response Time')\n",
    "        plt.ylabel('Seconds')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_history(self, filename: str = 'tutor_history.json') -> None:\n",
    "        \"\"\"\n",
    "        Save the question and response history to a file.\n",
    "        \n",
    "        Args:\n",
    "            filename: The filename to save to\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(self.history, f, indent=2)\n",
    "            console.print(f\"[bold green]History saved to {filename}[/bold green]\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error saving history:[/bold red] {str(e)}\")\n",
    "    \n",
    "    def load_history(self, filename: str = 'tutor_history.json') -> None:\n",
    "        \"\"\"\n",
    "        Load question and response history from a file.\n",
    "        \n",
    "        Args:\n",
    "            filename: The filename to load from\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.history = json.load(f)\n",
    "            console.print(f\"[bold green]History loaded from {filename}[/bold green]\")\n",
    "        except FileNotFoundError:\n",
    "            console.print(f\"[bold yellow]History file {filename} not found[/bold yellow]\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]Error loading history:[/bold red] {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4263949-58a1-4c55-8e67-18063dd99a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a tutor instance\n",
    "tutor = LLMTutor()\n",
    "console.print(\"[bold green]LLM Tutor initialized successfully![/bold green]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d721f8b3-9a3f-4d44-9611-dc5f8c7d0d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define your question here\n",
    "question = \"\"\"\n",
    "Given a list of dictionaries called 'books', write code to find and print all information \n",
    "about the book titled 'Mastery' by Robert Greene.\n",
    "\"\"\"\n",
    "\n",
    "console.print(Panel(f\"[bold]Question:[/bold]\\n{question}\", border_style=\"blue\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88220128-d73a-47d6-b936-20c68b753a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get responses from both models\n",
    "responses = tutor.ask(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a46e30d7-00d2-4f10-b1ad-520b220a3079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compare responses side by side\n",
    "tutor.compare_responses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7989d0e0-6035-428e-8e6a-3f344a947396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Show performance metrics\n",
    "tutor.show_performance_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814caf55-4723-485e-8c62-e8782b4ee8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save history to a file\n",
    "tutor.save_history(\"my_tutor_session.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4875c760-ebe8-4cfd-87aa-02845db655b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a new question\n",
    "new_question = \"Explain how to implement a binary search algorithm in Python.\"\n",
    "\n",
    "console.print(Panel(f\"[bold]New Question:[/bold]\\n{new_question}\", border_style=\"green\"))\n",
    "\n",
    "# Get responses for the new question\n",
    "new_responses = tutor.ask(new_question)\n",
    "\n",
    "# Compare responses\n",
    "tutor.compare_responses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "610f6174-4012-411b-b41f-b191a673263e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "simondb94-Improved-LLM-Tutor-",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
