{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "655d137f-6d1b-4bd9-a05c-1380e8fdb48a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# About Mini Project\n",
    "\n",
    "Mini project for hearing impaired people, using tools, suggesting songs according to a certain genre and in sign language. Speech to text converter with multiple language support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c023b71d-5339-4cbc-927d-93d22588bf6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extra requirements\n",
    "- pip install pydub simpleaudio speechrecognition pipwin pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b02d33a-5929-40b1-b421-9299192ca8b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de1ab7b6-42d7-4647-a4e0-81ed3852c92f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28c9c70-a287-4566-8d02-4468a92bc8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for hearing impaired people. \"\n",
    "system_message += \"Your mission is convert text to speech and speech to text. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "737f5554-f999-4c4a-a2be-a26740ddc6c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "songs_with_signs = {\n",
    "    \"electronic\": (\"God is a dj\", \"https://www.youtube.com/watch?v=bhSB8EEnCAM\", \"Faithless\"), \n",
    "    \"pop\": (\"Yitirmeden\", \"https://www.youtube.com/watch?v=aObdAXq1ZIo\", \"Pinhani\"), \n",
    "    \"rock\": (\"Bohemian Rhapsody\", \"https://www.youtube.com/watch?v=sjln9OMOw-0\", \"Queen\")\n",
    "}\n",
    "\n",
    "def get_songs_with_sign_language(genre):\n",
    "    print(f\"Tool get_songs_with_sign_language called for {genre}\")\n",
    "    city = genre.lower()\n",
    "    return songs_with_signs.get(genre, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7804eb58-ad06-4712-b8d9-02c8573b21f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_songs_with_sign_language(\"rock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a28cf49-294a-4acd-8bb5-1cbea727c4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "song_function = {\n",
    "    \"name\": \"get_songs_with_sign_language\",\n",
    "    \"description\": \"Get the corresponding song information for the specified given music genre. Call this whenever you need to know the songs with specific genre and in sign language, for example when a customer asks 'Suggest me sign language supported songs'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"genre\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The music genre that the customer wants to listen-watch to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"genre\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ebf0a4d-8a93-4361-aae8-05057f99c524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": song_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76e0579e-db9c-4535-90ad-c0a3f49e42fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    genre = arguments.get('genre')\n",
    "    song = get_songs_with_sign_language(genre)\n",
    "    song_info = song[2] + \": \"  + song[1]\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"genre\": genre,\"song\": song_info}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response, song[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b50d177b-b225-4e42-a566-cf943e977e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chat(history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "    genre = None\n",
    "    \n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response, genre = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "        \n",
    "    reply = response.choices[0].message.content\n",
    "    history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "    \n",
    "    return history, genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fe71ebf-5872-4353-b0b1-6cfdc3da8382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "import simpleaudio as sa\n",
    "\n",
    "def listener():\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening... Speak now!\")\n",
    "        recognizer.adjust_for_ambient_noise(source)  # Adjust for background noise\n",
    "        audio = recognizer.listen(source)\n",
    "    \n",
    "    try:\n",
    "        print(\"Processing speech...\")\n",
    "        text = recognizer.recognize_google(audio)  # Use Google Speech-to-Text\n",
    "        print(f\"You said: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, I could not understand what you said.\")\n",
    "        return None\n",
    "    except sr.RequestError:\n",
    "        print(\"Could not request results, please check your internet connection.\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "text = listener()  # Listen for speech\n",
    "if text:\n",
    "    print(f\"You just said: {text}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fba77656-03a9-40c3-9b8b-f377460bfb2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "convert = gr.State(False)\n",
    "def toggle_convert(current_value):\n",
    "    return not current_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caff68ed-1efb-4227-ab51-48b1d122d8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Tab(\"Chat\") as chat_interface:\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "            video = gr.HTML(f\"<a href=''> Example song will appear here </a>\")\n",
    "        with gr.Row():\n",
    "            entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "        with gr.Row():\n",
    "            clear = gr.Button(\"Clear\")\n",
    "    \n",
    "        def do_entry(message, history):\n",
    "            history += [{\"role\":\"user\", \"content\":message}]\n",
    "            return \"\", history\n",
    "    \n",
    "        entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "            chat, inputs=chatbot, outputs=[chatbot, video]\n",
    "        )\n",
    "        clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "    with gr.Tab(\"Speech to text converter\") as speech_to_text:\n",
    "        text_output = gr.Markdown(\"Press the button to start voice recognition\")\n",
    "        listen_button = gr.Button(\"Convert Voice to Text\")\n",
    "        language = gr.Dropdown([\"English\", \"Turkish\", \"Greek\", \"Arabic\"], label=\"Select output language\", value=\"English\")\n",
    "\n",
    "        def update_text(language):\n",
    "            \"\"\"Calls the listener and updates the markdown output in specific language.\"\"\"\n",
    "            text = listener()  # Replace with real speech-to-text function\n",
    "            system_prompt = f\"You are a useful translator. Convert text to {language}. Do not add aditional data, only translate it.\"\n",
    "            response = openai.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ],\n",
    "            )\n",
    "            return f\"**Converted Text:** {response.choices[0].message.content}\"\n",
    "\n",
    "        listen_button.click(update_text, inputs=[language], outputs=[text_output])\n",
    "\n",
    "ui.launch(inbrowser=True, share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c8b93be-d832-41c2-81e5-66a682653612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "day5-voice-to-text-converter-for-hearing-impaired-people",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
