{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "773245a9-e6c8-4276-8c16-13ef324a0f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ” Predicting Item Prices from Descriptions (Part 1)\n",
    "A complete pipeline from raw text to fine-tuned frontier and open source models\n",
    "\n",
    "---\n",
    "In this project, we aim to **predict item prices based solely on their textual descriptions**. \n",
    "\n",
    "We approach the problem with a structured 8-part pipeline:\n",
    "\n",
    "- ðŸ§© **Part 1: Data Curation & Preprocessing** : We aggregate, clean, analyze, and balance the dataset â€” then export it in .pkl format and save it in the HuggingFace Hub for the next step: model training and evaluation.\n",
    "\n",
    "- âš”ï¸ **Part 2: Traditional ML vs Frontier LLMs** : We compare traditional machine learning models (LR, SVR, XGBoost) using vectorized text inputs (BoW, Word2Vec) against LLMs like GPT-4o, LLaMA, Deepseek ... â— Who will predict better: handcrafted features or massive pretraining?\n",
    "\n",
    "- ðŸ§  **Part 3: E5 Embeddings & RAG** : We compare XGBoost on **contextual  dense embeddings** vs. Word2Vec, and test if **RAG** boosts GPT-4o Miniâ€™s price predictions. ðŸ“¦ Do contextual embeddings and retrieval improve price prediction?\n",
    "\n",
    "- ðŸ”§ **Part 4: Fine-Tuning GPT-4o Mini** : We fine-tune GPT-4o Mini on our curated dataset and compare performance before and after.\n",
    "ðŸ¤– Can a fine-tuned GPT-4o Mini beat its own zero-shot performance?\n",
    "\n",
    "- ðŸ¦™ **Part 5: Evaluating LLaMA 3.1 8B Quantized** : We run LLaMA 3.1 (8B, quantized) using the same evaluation setup to see how well an open-source base model performs with no fine-tuning.\n",
    "\n",
    "- âš™ï¸ **Part 6: Fine-Tuning LLaMA 3.1 with QLoRA** : We fine-tune LLaMA 3.1 using QLoRA and explore key hyperparameters, tracking **training and validation loss** to monitor overfitting and select the best configuration.\n",
    "\n",
    "- ðŸ§ª **Part 7: Evaluating Fine-Tuned LLaMA 3.1 8B (Quantized)** : After fine-tuning LLaMA 3.1, it's time to evaluate its performance and see how it stacks up against other models. Let's dive into the results.\n",
    "\n",
    "- ðŸ†**Part 8: Summary & Leaderboard** : Who comes out on top? Letâ€™s find out. We wrap up with final model rankings and key insights across ML, embeddings, RAG, and fine-tuned frontier and open-source models.\n",
    "\n",
    "---\n",
    "- âž¡ï¸ Data Curation & Preprocessing\n",
    "- Model Benchmarking â€“ Traditional ML vs LLMs\n",
    "- E5 Embeddings & RAG\n",
    "- Fine-Tuning GPT-4o Mini\n",
    "- Evaluating LLaMA 3.1 8B Quantized\n",
    "- Fine-Tuning LLaMA 3.1 with QLoRA\n",
    "- Evaluating Fine-Tuned LLaMA \n",
    "- Summary & Leaderboard\n",
    "\n",
    "---\n",
    "\n",
    "Letâ€™s begin with Part 1.\n",
    "\n",
    "# ðŸ§© Part 1: Data Curation & Preprocessing\n",
    "\n",
    "- Tasks:\n",
    "    - Load and filter dataset, then prepare each datapoint\n",
    "    - Explore, visualize, balance price distribution\n",
    "    - Export .pkl, upload to HF Hub\n",
    "- ðŸ§‘â€ðŸ’» Skill Level: Advanced\n",
    "- âš™ï¸ Hardware: âœ… CPU is sufficient â€” no GPU required\n",
    "- ðŸ› ï¸ Requirements: ðŸ”‘ Hugging Face Token\n",
    "\n",
    "---\n",
    "ðŸ“¢ Find more LLM notebooks on my [GitHub repository](https://github.com/lisekarimi/lexo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6f329f-0091-4961-a422-4d716ee1907d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!uv pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f4d5061-79c7-4b7b-beec-ef7fc66f311a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import importlib\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset, DatasetDict\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4244606f-d279-4b10-b0ef-0cca5df9d1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e6f95c-f8b7-4630-8351-5fc8b571c2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if not hf_token:\n",
    "   print(\"âŒ HF_TOKEN is missing\")\n",
    "\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac23451-8c61-4a2a-9b46-63a9bd6650cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âš™ï¸ Data Loading & Curation (Simultaneously)\n",
    "We load and curate the data at the same time using loaders.py and items.py.\n",
    "- Datasets come from: https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw/meta_categories\n",
    "- `loaders.py` handles parallel loading and filtering of products\n",
    "- `items.py` defines the Item class to clean, validate, and prepare each datapoint (title, description, price...) for modeling.\n",
    "\n",
    "\n",
    "ðŸ› ï¸ Note: Data is filtered to include items priced between 1 and 999 USD.\n",
    "\n",
    "ðŸ’¡ Comments have been added in both files to clarify the processing logic.\n",
    "\n",
    "âš ï¸ Loading 2.8M+ items can take 40+ mins on a regular laptop.\n",
    "\n",
    "âš ï¸ Set WORKER wisely in `loaders.py` to match your system capacity. Too many may crash your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84288d0-3cff-48d7-a5d3-c2678d18834d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('./helpers')\n",
    "import helpers.items\n",
    "import helpers.loaders\n",
    "\n",
    "importlib.reload(helpers.items)\n",
    "importlib.reload(helpers.loaders)\n",
    "\n",
    "from helpers.items import Item  # noqa: E402\n",
    "from helpers.loaders import ItemLoader  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf12b3d-849d-4e71-85d6-7f20bbc8dd23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"Automotive\",\n",
    "    \"Electronics\",\n",
    "    \"Office_Products\",\n",
    "    \"Tools_and_Home_Improvement\",\n",
    "    \"Cell_Phones_and_Accessories\",\n",
    "    \"Toys_and_Games\",\n",
    "    \"Appliances\",\n",
    "    \"Musical_Instruments\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2452249-77b1-4f1b-b309-f80bbf767347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "items = []\n",
    "for dataset_name in dataset_names:\n",
    "    loader = ItemLoader(dataset_name)\n",
    "    items.extend(loader.load())\n",
    "\n",
    "# Now, time for a coffee break!!\n",
    "# By the way, the larger datasets first... it speeds up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf63876-4378-418f-9107-46498c9802c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ” Inspecting a Sample Datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00765dea-46d3-4931-856d-a4b5df1004f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"A grand total of {len(items):,} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a54e1d6-4443-445c-8898-c32fb902cbd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Investigate the first item from the list\n",
    "\n",
    "datapoint = items[0]\n",
    "\n",
    "# Access various attributes\n",
    "title = datapoint.title\n",
    "details = datapoint.details\n",
    "price = datapoint.price\n",
    "category = datapoint.category\n",
    "\n",
    "print(f\"Datapoint: {datapoint}\")\n",
    "print('*' * 40)\n",
    "print(f\"Title: {title}\")\n",
    "print('*' * 40)\n",
    "print(f\"Detail: {details}\")\n",
    "print('*' * 40)\n",
    "print(f\"Price: ${price}\")\n",
    "print('*' * 40)\n",
    "print(f\"Category: {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77e67f7-dda4-47ce-b5fe-8bc8affa8038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The prompt that will be used during training\n",
    "print(items[0].prompt)\n",
    "print('*' * 40)\n",
    "# The prompt that will be used during testing\n",
    "print(items[0].test_prompt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f06a06-8409-442b-bf9e-4fe14bfe9f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“Š Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60ca8cd5-ea5b-4f08-9148-2b6df6968e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12ccf0a6-0170-46af-a518-3f61b0d4945e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of token counts\n",
    "\n",
    "tokens = [item.token_count for item in items]\n",
    "plt.title(f\"Token counts: Avg {sum(tokens)/len(tokens):,.1f} and highest {max(tokens):,}\\n\")\n",
    "plt.xlabel('Length (tokens)')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(tokens, rwidth=0.7, color=\"blue\", bins=range(0, 300, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29c7d515-4526-4006-a198-ce1388abd035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a854589c-31e2-491c-a01c-0648a8854f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of prices\n",
    "\n",
    "prices = [item.price for item in items]\n",
    "plt.title(f\"Prices: Avg {sum(prices)/len(prices):,.1f} and highest {max(prices):,}\\n\")\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(prices, rwidth=0.7, color=\"blueviolet\", bins=range(0, 1000, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf5f0fe-c3ef-425a-94db-f334f901abee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of categories\n",
    "\n",
    "category_counts = Counter()\n",
    "for item in items:\n",
    "    category_counts[item.category]+=1\n",
    "\n",
    "categories = category_counts.keys()\n",
    "counts = [category_counts[category] for category in categories]\n",
    "\n",
    "# Bar chart by category\n",
    "plt.bar(categories, counts, color=\"goldenrod\")\n",
    "plt.title('How many items in each category')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v, f\"{v:,}\", ha='center', va='bottom')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10af7464-c7ee-4d32-8988-75c9419947c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸŽ¯ Data Sampling\n",
    "\n",
    "We sample to keep the dataset balanced but rich:\n",
    "- ðŸŽ¯ Keep all items if price â‰¥ $240 or group size â‰¤ 1200\n",
    "- ðŸŽ¯ For large groups, randomly sample 1200 items, favoring rare categories\n",
    "\n",
    "âœ… This keeps valuable high-price items and avoids overrepresented classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1632e673-d890-48c1-82e1-9722170ae9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "HEAVY_DATASET = \"Automative\"\n",
    "\n",
    "# Group items by rounded price\n",
    "# Slots is a dictionary where the keys are rounded prices and the values are lists of items that have that rounded price\n",
    "slots = defaultdict(list)\n",
    "for item in items:\n",
    "    slots[round(item.price)].append(item)\n",
    "\n",
    "np.random.seed(42) # Set random seed for reproducibility\n",
    "sample = [] # Final collection of items after our sampling process completes\n",
    "\n",
    "# Sampling loop\n",
    "for price, items_at_price in slots.items():\n",
    "\n",
    "    # Take all items if price â‰¥ 240 or small group\n",
    "    if price >= 240 or len(items_at_price) <= 1200:\n",
    "        sample.extend(items_at_price)\n",
    "\n",
    "    # Otherwise sample 1200 items with weights\n",
    "    else:\n",
    "\n",
    "        # Weight: 1 for toys, 5 for others\n",
    "        weights = [1 if item.category == HEAVY_DATASET else 5 for item in items_at_price]\n",
    "        weights = np.array(weights) / sum(weights)\n",
    "\n",
    "        indices = np.random.choice(len(items_at_price), 1200, False, weights) # False = don't pick the same index twice\n",
    "        sample.extend([items_at_price[i] for i in indices])\n",
    "\n",
    "print(f\"There are {len(sample):,} items in the sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d479199-1c0f-441b-94d3-d49233b16a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of prices in the sample\n",
    "\n",
    "prices = [float(item.price) for item in sample]\n",
    "plt.title(f\"Avg {sum(prices)/len(prices):.2f} and highest {max(prices):,.2f}\\n\")\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(prices, rwidth=0.7, color=\"darkblue\", bins=range(0, 1000, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22165ff2-f9ad-4509-be86-4d92d80cbfcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of categories in the sample\n",
    "\n",
    "category_counts = Counter()\n",
    "for item in sample:\n",
    "    category_counts[item.category]+=1\n",
    "\n",
    "categories = category_counts.keys()\n",
    "counts = [category_counts[category] for category in categories]\n",
    "\n",
    "# Create bar chart\n",
    "plt.bar(categories, counts, color=\"pink\")\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('How many in each category')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(i, v, f\"{v:,}\", ha='center', va='bottom')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fd2b0f2-bf83-45c0-8730-f8ea44c088c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The HEAVY_DATASET still in the lead, but improved somewhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b7959eb-b174-4f89-acc4-9f407bb28fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create pie chart\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    counts,\n",
    "    # labels=categories,\n",
    "    autopct='%1.0f%%',\n",
    "    startangle=90,\n",
    "    pctdistance=0.85,\n",
    "    labeldistance=1.1\n",
    ")\n",
    "ax.legend(wedges, categories, title=\"Categories\", loc=\"lower center\", bbox_to_anchor=(0.5, 1.15), ncol=3)\n",
    "\n",
    "# Draw donut center\n",
    "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Add center label\n",
    "ax.text(0, 0, \"Categories\", ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Equal aspect ratio\n",
    "plt.axis('equal')\n",
    "plt.title(\"Category Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8061fd7e-d550-47ca-8e70-a02d7a6ab314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How does the price vary with the character count of the prompt?\n",
    "\n",
    "sizes = [len(item.prompt) for item in sample]\n",
    "prices = [item.price for item in sample]\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.scatter(sizes, prices, s=0.2, color=\"red\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Is there a simple correlation between prompt length and item price?')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05b9541d-048e-4a95-9356-c8e5568b11a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There is no strong or simple correlation between prompt length and item price.\n",
    "\n",
    "In other words, longer prompts donâ€™t clearly mean higher prices, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a922643-05d0-4a8b-acfa-8493b78fa7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## âœ… Final Check Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fdf2fcc-5af7-4eae-b63c-1fce6f3a2975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the price label is correctly placed by the end of the prompt\n",
    "\n",
    "def report(item):\n",
    "    prompt = item.prompt\n",
    "    tokens = Item.tokenizer.encode(item.prompt)\n",
    "    print(prompt)\n",
    "    print(tokens[-6:])\n",
    "    print(Item.tokenizer.batch_decode(tokens[-6:]))\n",
    "\n",
    "report(sample[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9386518-5513-4a68-95b2-411e6e10dbc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "LLaMA and GPT-4o both tokenize numbers from 1 to 999 as a single token, while models like Qwen2, Gemma, and Phi-3 split them into multiple tokens. This helps keep prices compact in our prompts â€” useful for our project, though not strictly required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19ed344e-8d77-4573-bf71-bc2fb8e892a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“¦ Creating Train/Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81b2daf5-b219-4220-a31f-750f2a96a140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(sample)\n",
    "train = sample[:400_000]\n",
    "test = sample[400_000:402_000]\n",
    "print(f\"Divided into a training set of {len(train):,} items and test set of {len(test):,} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a23792d-cb6b-4c83-a360-4e25a8dc31a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(train[0].prompt)\n",
    "print('*' * 40)\n",
    "print(test[0].test_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1d32631-734c-4485-81bc-c46a60531b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of prices in the first 250 test points\n",
    "\n",
    "prices = [float(item.price) for item in test[:250]]\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.title(f\"Avg {sum(prices)/len(prices):.2f} and highest {max(prices):,.2f}\\n\")\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(prices, rwidth=0.7, color=\"darkblue\", bins=range(0, 1000, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "608b6843-9331-4c48-a205-fb9cf39c69ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Extract prompts and prices\n",
    "train_prompts = [item.prompt for item in train]\n",
    "train_prices = [item.price for item in train]\n",
    "test_prompts = [item.test_prompt() for item in test]\n",
    "test_prices = [item.price for item in test]\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_prompts, \"price\": train_prices})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_prompts, \"price\": test_prices})\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save full Item objects\n",
    "os.makedirs(\"data\", exist_ok=True) # Make sure the folder exists\n",
    "\n",
    "# Save full Item objects to the folder\n",
    "with open('data/train.pkl', 'wb') as file:\n",
    "    pickle.dump(train, file)\n",
    "\n",
    "with open('data/test.pkl', 'wb') as file:\n",
    "    pickle.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f11dd8a-58cc-43ab-b1af-b3b6a4f97eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Push to the Hugging Face Hub\n",
    "USERNAME = \"lisekarimi\"  # ðŸ”§ Replace with your Hugging Face username\n",
    "DATASET_NAME = f\"{USERNAME}/pricer-data\"\n",
    "\n",
    "dataset.push_to_hub(DATASET_NAME, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c4e23ea-42f7-439b-adde-452628dc1b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note:** \n",
    "- The dataset `pricer-data` on Hugging Face only contains `text` and `price`:\n",
    "\n",
    "\n",
    "{\n",
    "  \"text\": \"How much does this cost...Price is $175.00\",\n",
    "  \"price\": 175.0\n",
    "}\n",
    "\n",
    "- Full `Item` objects (with metadata) are available in `train.pkl` and `test.pkl`:\n",
    "\n",
    "Item(data={\n",
    "    \"title\": str,\n",
    "    \"description\": list[str],\n",
    "    \"features\": list[str],\n",
    "    \"details\": str\n",
    "}, price=float)\n",
    "\n",
    "\n",
    "Now, itâ€™s time to move on to **Part 2: Model Benchmarking â€“ Traditional ML vs Frontier LLMs.**\n",
    "\n",
    "ðŸ”œ See you in the [next notebook](https://github.com/lisekarimi/lexo/blob/main/09_part2_tradml_vs_frontier.ipynb)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "09_part1_data_curation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
